{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Jupyter Notebooks\n",
    "\n",
    "This file is a \"Jupyter Notebook\". Jupyter Notebooks are files that allow one to write and evaluate Python (and R, and Julia...) alongside documentation, which makes them great for exploratory code investigations.\n",
    "\n",
    "To run this notebook locally on your machine, we recommend that you follow these steps.\n",
    "\n",
    "### Installing Anaconda (Optional)\n",
    "\n",
    "To follow along, the first step will be to [install Anaconda](https://www.anaconda.com/download/), a distribution of the Python programming language that helps make managing Python easier.\n",
    "\n",
    "Once Anaconda is installed, open a new terminal window. (If you are on Windows, you should open an Anaconda terminal by going to Programs -> Anaconda3 (64-bit) -> Anaconda Prompt). Then you can create and activate a virtual environment:\n",
    "\n",
    "```\n",
    "# create a virtual environment with Python 3.6 named \"3.6\"\n",
    "conda create python=3.6 --name=3.6\n",
    "\n",
    "# activate the virtual environment\n",
    "source activate 3.6\n",
    "```\n",
    "\n",
    "### Running the Workshop Notebook\n",
    "\n",
    "You should now see `(3.6)` prepended on your path. Once you see that prefix, you can start the notebook with the following commands:\n",
    "\n",
    "```\n",
    "git clone https://github.com/YaleDHLab/lab-workshops\n",
    "cd lab-workshops/machine-learning\n",
    "pip install -r requirements.txt\n",
    "jupyter notebook machine-learning.ipynb\n",
    "```\n",
    "\n",
    "Once the notebook is open, you can evaluate a code cell by clicking on that cell, then clicking `Cell -> Run Cells`. Alternatively, after clicking on a cell you can hold Control and press Enter to execute the code in the cell. To run all the cells in a notebook (which I recommend you do for this notebook), you can click `Cell -> Run All`.\n",
    "\n",
    "If you want to add a new cell, click the \"<b>+</b>\" button near the top of the page (below and between File and Edit). In that new cell, you can type Python code, like `import this`, then run the cell and immediately see the output. I encourage you to add and modify cells as we move through the discussion of machine learning below, as interacting with the code is one of the best ways to grow comfortable with the techniques discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Word vectors\", or \"word embeddings\" are ways of representing semantic and syntactic information about words in a vector form. As you may know, the word \"vector\" is just a fancy term for a list of numbers (or sometimes a list of number lists). Word vectors are a way of representing each word in a vocabulary with a list of numbers, such that those numbers can tell us useful information about the words in the vocabulary.\n",
    "\n",
    "The simplest kind of word vector system represents each distinct word in a vocabulary with a `w` dimensional vector (or list of `w` numbers), where `w` equals the number of distinct words in the language. Each word's vector consists of zeros for all but the `i-th` value in the list, where `i` indicates the given word's index position within the vocabulary. This is known as a \"[one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f),\" as the vector contains 0s in all but one position.\n",
    "\n",
    "For example, suppose one has a vocabulary consisting only of five words: King, Queen, Man, Woman, and Child. In that case one could encode the word \"Queen\" as:\n",
    "\n",
    "<img src='images/word2vec-one-hot.png' style='width: 600px'>\n",
    "\n",
    "<div style='text-align: center'><a href='https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/'>Image by Adrian Coyler</a></div>\n",
    "\n",
    "While it's easy to represent words with this kind of one-hot encoding, these vectors don't give us any way to compare words except to check if they're equal, which isn't very helpful.\n",
    "\n",
    "Recent approaches to word vectors, including Google's Word2Vec and Stanford University's GloVe embeddings, create more insightful word vectors by representing each word in a language with a dense `k` dimensional vector, where `k` is a value chosen by the user who creates the word vectors. (Dense vectors are lists of numbers in which there are very few if any zeros; unlike one-hot vectors which are comprised of all zeros with a single 1, dense vectors are comprised almost entirely of non-zero values). The meaning of each word is thereby represented by a list of `k` values, and each unit in the `k` dimensions contributes some meaning to each word.\n",
    "\n",
    "If one were to label the dimensions in a word vector, the result might look something like the following:\n",
    "\n",
    "<img src='images/word2vec-distributed-representation.png' style='width: 600px'>\n",
    "\n",
    "<div style='text-align: center'><a href='https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/'>Image by Adrian Coyler</a></div>\n",
    "\n",
    "Each of these word vectors (the columns above) gives a representation of the semantic and syntactic function of a word in a language. By comparing these vectors, we can study relationships between words in ways that were previously not possible. To see how this works, let's dive into some code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Code\n",
    "\n",
    "The following block loads some dependencies, clears all warnings, and makes random number generation reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 # reimport modules when evaluating cells\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore all warnings\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0) # make random number generation consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pretrained Word Vectors\n",
    "\n",
    "The following section will allow us to get started with word vectors by loading a pretrained model trained by Google. This model has already learned the mapping from each word to a 300 dimensional vector, so we won't need to \"train\" this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "\n",
    "# step one: download (1.5GB) Google's model to your current directory\n",
    "url = 'https://s3.amazonaws.com/lab-data-collections/GoogleNews-vectors-negative300.bin.gz'\n",
    "open('GoogleNews-vectors-negative300.bin.gz', 'wb').write(requests.get(url, allow_redirects=True).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# step two: unzip the gzipped model file we just downloaded\n",
    "with gzip.open('GoogleNews-vectors-negative300.bin.gz', 'rb') as f:\n",
    "  with open('pretrained-model.bin', 'wb') as out:\n",
    "    for i in f:\n",
    "      out.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# step three: load the pretrained model we just downloaded and unzipped\n",
    "model = KeyedVectors.load_word2vec_format('pretrained-model.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Model\n",
    "\n",
    "Now that we've downloaded and loaded a pretrained model, let's see what this model can do.\n",
    "\n",
    "To do so, let's use one of the most useful Python commands, `dir()`, which displays all of the attributes (data appended to object) and methods (functions we can call on an object) defined on the model we just instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate all attributes on the model\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two attributes that are of particular interest in this list: `vocab` and `wv`. The former lists all of the words in the model, while the latter lets us fetch the vector associated with a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.vocab)[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv['tiptop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these attributes below to investigate the model more thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Word Vectors\n",
    "\n",
    "Now that we've downloaded and loaded a pretrained model, let's use it to compare how similar words are. We should expect words we associate with one-another to have high similarity values, and words that we don't associate together to have low similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "# get the vectors associated with two sample words\n",
    "vec_a = model.wv['beach']\n",
    "vec_b = model.wv['ocean']\n",
    "\n",
    "# find the n most similar terms to a given query vector\n",
    "print(model.wv.similar_by_vector(vec_a, topn=3))\n",
    "\n",
    "# show the distance between those vectors\n",
    "print(cosine(vec_a, vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you try to get the vector associated with a word that doesn't exist, an error springs!\n",
    "word = 'cats_in_trees'\n",
    "\n",
    "try:\n",
    "  model.wv[word]\n",
    "except KeyError:\n",
    "  print(' ! Word missing from vocabulary!', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all words in the model\n",
    "words = list(model.wv.vocab.keys())\n",
    "\n",
    "words[:100] # show just the first hundred words in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Word Vector Fields\n",
    "\n",
    "In the code above, we examined the simialarity between two word vectors. In what follows, we'll visualize the similarity between hundreds of words at once. To do so, we'll reduce the vector representation of each word to just two dimensions, then we'll create a visualization that renders each word at its two-dimensional position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# visualize the field of terms fed as input\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "def plot_word_field(model, method='umap', skip=0, max_words=1000, jitter=0, margin=0.05, figsize=(20, 14), min_dist=0.1):\n",
    "  '''\n",
    "  Given a gensim.model instance, plot the positions of terms\n",
    "  '''\n",
    "  if method not in ['tsne', 'umap']: raise Exception(' ! Requested method is not supported')\n",
    "  words = get_model_words(model, max_words=max_words, skip=skip)\n",
    "  if not words: raise Exception(' ! No words were found in model -- exiting')\n",
    "  word_vectors = np.array([model.wv[word] for word in words]) # array of vecs, one per word\n",
    "  print(' * creating layout for', len(word_vectors), 'words')\n",
    "  if method == 'umap':\n",
    "    X = UMAP(n_neighbors=5, min_dist=min_dist).fit_transform(word_vectors) # X.shape = len(words), 2\n",
    "  elif method == 'tsne':\n",
    "    X = TSNE(n_components=2).fit_transform(word_vectors) # X.shape = len(words), 2\n",
    "  plot_words(X, words, figsize=figsize, jitter=jitter, margin=margin)\n",
    "\n",
    "  \n",
    "def get_model_words(model, max_words=1000, skip=0):\n",
    "  '''Get the words from `model`. If `max_words` is provided, return up to that many words'''\n",
    "  words = list(model.wv.vocab.keys()) # find all words in the model\n",
    "  if max_words: # get the n most popular words if user requested max words\n",
    "    word_to_count = {w: model.wv.vocab[w].count for w in words}\n",
    "    sorted_by_count = sorted(word_to_count.items(), key=operator.itemgetter(1))\n",
    "    sorted_by_count.reverse()\n",
    "    words = [i[0] for i in sorted_by_count[skip:skip+max_words]]\n",
    "  return words\n",
    "\n",
    "\n",
    "def plot_words(X, words, jitter=False, figsize=(10,6), margin=0.5, labels={}):\n",
    "  '''Given `X` where shape = words,2 and list of strings `words` plot the words at positions in `X`'''\n",
    "  plt.figure(figsize=figsize)\n",
    "  words = [plt.text(X[word_idx][0], X[word_idx][1], word) for word_idx, word in enumerate(words)]\n",
    "  if jitter: adjust_text(words, lim=jitter)\n",
    "  if labels.get('x', False): plt.xlabel(labels['x'])\n",
    "  if labels.get('y', False): plt.ylabel(labels['y'])\n",
    "  # set the axis ranges\n",
    "  x_vals = [w.get_position()[0] for w in words]\n",
    "  y_vals = [w.get_position()[1] for w in words]\n",
    "  plt.xlim(( min(x_vals)-margin, max(x_vals)+margin ))\n",
    "  plt.ylim(( min(y_vals)-margin, max(y_vals)+margin ))\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = plot_word_field(model, skip=5000, max_words=500, jitter=0, min_dist=0.25, method='umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Approximate Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def build_ann_index(model):\n",
    "  '''Build an ANN model and persist to disk for faster vector similarity queries'''\n",
    "  words = list(model.wv.vocab.keys()) # list of strings, one per word\n",
    "  idx_to_word = {str(idx): i for idx, i in enumerate(words)} # d[word] = word_idx in words\n",
    "  dims = model.wv[words[0]].shape[0] # number of dimensions in each input vector\n",
    "\n",
    "  # create the approximate nearest neighbors model\n",
    "  if not os.path.exists('model.ann'):\n",
    "    ann = AnnoyIndex(dims)\n",
    "    for i in idx_to_word:\n",
    "      ann.add_item(int(i), model.wv[idx_to_word[i]])\n",
    "    ann.build(10) # number of 'trees' to build\n",
    "    ann.save('model.ann')\n",
    "  if not os.path.exists('idx_to_word.json'):\n",
    "    with open('idx_to_word.json', 'w') as out: json.dump(idx_to_word, out)\n",
    "\n",
    "  # load the saved model\n",
    "  ann = AnnoyIndex(dims)\n",
    "  ann.load('model.ann')\n",
    "  idx_to_word = json.load(open('idx_to_word.json'))\n",
    "  \n",
    "  # return the model for querying and the map from word to idx\n",
    "  return ann, idx_to_word\n",
    "\n",
    "\n",
    "def find_centroid(words):\n",
    "  '''Given a list of words, get the centroid of those word's vectors'''\n",
    "  vecs = np.vstack([model.wv[w] for w in words if w in model.wv])\n",
    "  sums = np.array([ np.sum(vecs[:,idx]) for idx, i in enumerate(range(vecs[0].shape[0])) ])\n",
    "  return sums / vecs[0].shape[0]\n",
    "\n",
    "\n",
    "def find_similar_by_vec(vec, n=50):\n",
    "  '''Return the words for the `n` most similar words to a query vector'''\n",
    "  indices = ann.get_nns_by_vector(vec, n*2**2, search_k=-1, include_distances=False)\n",
    "  similar_words = [idx_to_word[str(i)] for i in indices]\n",
    "  curated = []\n",
    "  for idx, word in enumerate(similar_words):\n",
    "    if len(curated) < n:\n",
    "      if similar_words[idx].lower() not in curated:\n",
    "        curated.append(similar_words[idx].lower())\n",
    "  return curated\n",
    "\n",
    "\n",
    "def find_similar_by_words(words, n=50):\n",
    "  '''Return the words for the `n` most similar words to a list of query words'''\n",
    "  centroid = find_centroid(words)\n",
    "  indices = ann.get_nns_by_vector(centroid, n*2**2, search_k=-1, include_distances=False)\n",
    "  similar_words = [idx_to_word[str(i)] for i in indices]\n",
    "  curated = []\n",
    "  for idx, word in enumerate(similar_words):\n",
    "    if len(curated) < n:\n",
    "      if similar_words[idx].lower() not in words and similar_words[idx].lower() not in curated:\n",
    "        curated.append(similar_words[idx].lower())\n",
    "  return curated\n",
    "\n",
    "\n",
    "# prepare data structures that will expedite the process of finding words similar to a query vector\n",
    "ann, idx_to_word = build_ann_index(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Terms Related to a Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of words in a conceptual category        \n",
    "veggie_words = ['asparagus', 'artichoke', 'avocado', 'beets', 'broccoli', 'carrot', 'celery', 'cauliflower', 'cucumber', 'eggplant', 'kale', 'lentils', 'lettuce', 'mushroom', 'olive', 'onion', 'pea', 'potato', 'salad', 'spinach', 'squash', 'tomato', 'turnip', 'yam', 'zucchini']\n",
    "meat_words = ['bacon', 'beef', 'chicken', 'crab', 'duck', 'goose', 'meat', 'meatball', 'mutton', 'offal', 'partridge', 'pheasant', 'pork', 'quail', 'rabbit', 'turkey', 'veal', 'venison']\n",
    "\n",
    "other_veggie_words = find_similar_by_words(veggie_words)\n",
    "other_meat_words = find_similar_by_words(meat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_meat_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting relationships between concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as dist\n",
    "\n",
    "def plot_distances_to_concepts(words, vec_one, vec_two, jitter=0, labels={}):\n",
    "  '''Plot each item in `words` accordint to its distance from `vec_one` and `vec_two`'''\n",
    "  # 2D array where subarrays = [dist_from_vec_one, dist_from_vec_two]\n",
    "  words = [w for w in words if w in model.wv]\n",
    "  distances = [[1-dist(model.wv[w], vec_one), 1-dist(model.wv[w], vec_two)] for w in words]\n",
    "  plot_words(np.vstack(distances), words, jitter=jitter, labels=labels) \n",
    "\n",
    "# find the centroids for each cluster of words\n",
    "veggie_centroid = find_centroid(veggie_words)\n",
    "meat_centroid = find_centroid(meat_words)\n",
    "\n",
    "# plot the words in the concept space\n",
    "words = other_veggie_words + other_meat_words\n",
    "plot_distances_to_concepts(words, veggie_centroid, meat_centroid, jitter=4, labels={'x': 'vegginess', 'y': 'meatiness'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Word2Vec Models with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import requests as r\n",
    "import os\n",
    "\n",
    "def download_text(url, out_dir='texts'):\n",
    "  '''Download the content at `url` to the present directory'''\n",
    "  if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "  response = r.get(url, allow_redirects=True)\n",
    "  out_path = os.path.join(out_dir, os.path.basename(i))\n",
    "  open(out_path, 'wb').write(response.content)\n",
    "  print(' * downloaded', url)\n",
    "\n",
    "# specify the urls to one or more text files\n",
    "urls = [\n",
    "  'http://www.gutenberg.org/cache/epub/14/pg14.txt',\n",
    "  'http://www.gutenberg.org/cache/epub/25/pg25.txt',\n",
    "  'http://www.gutenberg.org/cache/epub/48/pg48.txt',\n",
    "]\n",
    "\n",
    "# download each of the text files specified above\n",
    "for i in urls: download_text(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "\n",
    "# generate a list of lists, where sublists contain words from a file (in order)\n",
    "word_lists = []\n",
    "for i in glob.glob('texts/*'):\n",
    "  with codecs.open(i) as f:\n",
    "    word_lists.append(f.read().lower().split())\n",
    "\n",
    "# build a model with the custom word lists\n",
    "model = Word2Vec(word_lists, size=100, window=5, min_count=20, workers=4)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the words\n",
    "plot_word_field(model, method='umap', min_dist=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Word Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "max_words = 1000\n",
    "skip = 1000\n",
    "\n",
    "words = get_model_words(model, max_words=max_words, skip=skip)\n",
    "vecs = [model.wv[i] for i in words]\n",
    "\n",
    "Z = linkage(vecs, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(max_words/4, 8))\n",
    "dendrogram(Z, leaf_rotation=90.0, leaf_font_size=14.0, labels=words)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
