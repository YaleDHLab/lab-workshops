{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Jupyter Notebooks\n",
    "\n",
    "This file is a \"Jupyter Notebook\". Jupyter Notebooks are files that allow one to write and evaluate Python (and R, and Julia...) alongside documentation, which makes them great for exploratory code investigations.\n",
    "\n",
    "To run this notebook locally on your machine, we recommend that you follow these steps.\n",
    "\n",
    "### Installing Anaconda (Optional)\n",
    "\n",
    "To follow along, the first step will be to [install Anaconda](https://www.anaconda.com/download/), a distribution of the Python programming language that helps make managing Python easier.\n",
    "\n",
    "Once Anaconda is installed, open a new terminal window. (If you are on Windows, you should open an Anaconda terminal by going to Programs -> Anaconda3 (64-bit) -> Anaconda Prompt). Then you can create and activate a virtual environment:\n",
    "\n",
    "```\n",
    "# create a virtual environment with Python 3.6 named \"3.6\"\n",
    "conda create python=3.6 --name=3.6\n",
    "\n",
    "# activate the virtual environment\n",
    "source activate 3.6\n",
    "```\n",
    "\n",
    "### Running the Workshop Notebook\n",
    "\n",
    "You should now see `(3.6)` prepended on your path. Once you see that prefix, you can start the notebook with the following commands:\n",
    "\n",
    "```\n",
    "git clone https://github.com/YaleDHLab/lab-workshops\n",
    "cd lab-workshops/word-vectors\n",
    "pip install -r requirements.txt\n",
    "jupyter notebook word-vectors.ipynb\n",
    "```\n",
    "\n",
    "Once the notebook is open, you can evaluate a code cell by clicking on that cell, then clicking `Cell -> Run Cells`. Alternatively, after clicking on a cell you can hold Control and press Enter to execute the code in the cell. To run all the cells in a notebook (which I recommend you do for this notebook), you can click `Cell -> Run All`.\n",
    "\n",
    "If you want to add a new cell, click the \"<b>+</b>\" button near the top of the page (below and between File and Edit). In that new cell, you can type Python code, like `import this`, then run the cell and immediately see the output. I encourage you to add and modify cells as we move through the discussion of machine learning below, as interacting with the code is one of the best ways to grow comfortable with the techniques discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases and Motivations for Studying Word Vectors\n",
    "\n",
    "Word vectors are data structures that map sequences of alphabetic characters (aka \"words\") to lists of numbers (aka \"vectors\"). These peculiar data structures have been used in all sorts of research applications, and are certainly worth exploring if your work touches on any of the following domains:\n",
    "\n",
    " * <b>Multilingual and Cross-lingual analysis</b>: If you work on works in translation, or on the influence of writers who write in one language on those who write in another language, word vectors can valuable ways to study these kinds of cross-lingual relationships algorithmically.  \n",

    "  [[Case Study: Using word vectors to study endangered languages](https://raw.githubusercontent.com/YaleDHLab/lab-workshops/master/word-vectors/papers/coeckelbergs.pdf)]\n",
    " * <b>Studying Language Change over Time</b>: If you want to study the way the meaning of a word has changed over time, word vectors provide an exceptional method for this kind of study.  \n",
    "  [[Case Study: Using word vectors to analyze the changing meaning of the word \"gay\" in the twentieth century.](https://nlp.stanford.edu/projects/histwords/)]\n",
    " * <b>Analyzing Historical Concept Formation</b>: If you want to analyze the ways writers in a given historical period understood particular concepts like \"honor\" and \"chivalry\", then word vectors can provide excellent opportunities to uncover these hidden associations.  \n",
    " [[Case Study: Using word vectors to study the ways eighteenth-century authors organized moral abstractions](https://raw.githubusercontent.com/YaleDHLab/lab-workshops/master/word-vectors/papers/heuser.pdf)]\n",
    " * <b>Uncovering Text Reuse</b>: If you want to study text reuse or literary imitation (either within one language or across multiple languages), word vectors can provide excellent tools for identifying similar passages of text.  \n",
    " [[Case Study: Using word vectors to uncover cross-lingual text reuse in eighteenth-century writing](https://douglasduhaime.com/posts/crosslingual-plagiarism-detection.html)]\n",
    "\n",
    "These are of course only a few of the many possible use cases for word vectors. The field is still new, so there are plenty of new applications you can discover!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Word vectors\", or \"word embeddings\" are ways of representing semantic and syntactic information about words in a vector form. As you may know, the word \"vector\" is just a fancy term for a list of numbers (or sometimes a list of number lists). Word vectors are a way of representing each word in a vocabulary with a list of numbers, such that those numbers can tell us useful information about the words in the vocabulary.\n",
    "\n",
    "The simplest kind of word vector system represents each distinct word in a vocabulary with a `w` dimensional vector (or list of `w` numbers), where `w` equals the number of distinct words in the language. Each word's vector consists of zeros for all but the `i-th` value in the list, where `i` indicates the given word's index position within the vocabulary. This is known as a \"[one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f),\" as the vector contains 0s in all but one position.\n",
    "\n",
    "For example, suppose one has a vocabulary consisting only of five words: King, Queen, Man, Woman, and Child. In that case one could encode the word \"Queen\" as:\n",
    "\n",
    "<img src='images/word2vec-one-hot.png' style='width: 600px'>\n",
    "\n",
    "<div style='text-align: center'><a href='https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/'>Image by Adrian Coyler</a></div>\n",
    "\n",
    "While it's easy to represent words with this kind of one-hot encoding, these vectors don't give us any way to compare words except to check if they're equal, which isn't very helpful.\n",
    "\n",
    "Recent approaches to word vectors, including Google's Word2Vec and Stanford University's GloVe embeddings, create more insightful word vectors by representing each word in a language with a dense `k` dimensional vector, where `k` is a value chosen by the user who creates the word vectors. (Dense vectors are lists of numbers in which there are very few if any zeros; unlike one-hot vectors which are comprised of all zeros with a single 1, dense vectors are comprised almost entirely of non-zero values). The meaning of each word is thereby represented by a list of `k` values, and each unit in the `k` dimensions contributes some meaning to each word.\n",
    "\n",
    "If one were to label the dimensions in a word vector, the result might look something like the following:\n",
    "\n",
    "<img src='images/word2vec-distributed-representation.png' style='width: 600px'>\n",
    "\n",
    "<div style='text-align: center'><a href='https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/'>Image by Adrian Coyler</a></div>\n",
    "\n",
    "Each of these word vectors (the columns above) gives a representation of the semantic and syntactic function of a word in a language. By comparing these vectors, we can study relationships between words in ways that were previously not possible. To see how this works, let's dive into some code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Code\n",
    "\n",
    "The following block loads some dependencies, clears all warnings, and makes random number generation reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 # reimport modules when evaluating cells\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore all warnings\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0) # make random number generation consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pretrained Word Vectors\n",
    "\n",
    "The following section will allow us to get started with word vectors by loading a pretrained model trained by Google. This model has already learned the mapping from each word to a 300 dimensional vector, so we won't need to \"train\" this model. The download will take a minute or two, but that's much faster than training a model from scratch, which could take days or weeks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "# step one: download (1.5GB) Google's model to your current directory\n",
    "if not os.path.exists('GoogleNews-vectors-negative300.bin.gz'):\n",
    "  url = 'https://s3.amazonaws.com/lab-data-collections/GoogleNews-vectors-negative300.bin.gz'\n",
    "  open('GoogleNews-vectors-negative300.bin.gz', 'wb').write(requests.get(url, allow_redirects=True).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# step two: unzip the gzipped model file we just downloaded\n",
    "if not os.path.exists('pretrained-model.bin'):\n",
    "  with gzip.open('GoogleNews-vectors-negative300.bin.gz', 'rb') as f:\n",
    "    with open('pretrained-model.bin', 'wb') as out:\n",
    "      for i in f:\n",
    "        out.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# step three: load the pretrained model we just downloaded and unzipped\n",
    "model = KeyedVectors.load_word2vec_format('pretrained-model.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vec = model.wv['tiptop']\n",
    "\n",
    "words, sims = zip(*model.wv.similar_by_vector(vec, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Model\n",
    "\n",
    "Now that we've downloaded and loaded a pretrained model, let's see what this model can do.\n",
    "\n",
    "To do so, let's use one of the most useful Python commands, `dir()`, which displays all of the attributes (data appended to object) and methods (functions we can call on an object) defined on the model we just instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# investigate all attributes on the model\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two attributes that are of particular interest in this list: `vocab` and `wv`. The former lists all of the words in the model, while the latter lets us fetch the vector associated with a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(list(model.vocab)[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model.wv['tiptop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these attributes below to investigate the model more thoroughly. (For information on some of the other attributes of `model` listed above, check out the [Gensim documentation](https://github.com/RaRe-Technologies/gensim))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Word Vectors\n",
    "\n",
    "Now that we've downloaded and loaded a pretrained model, let's use it to compare how similar words are. We should expect words we associate with one-another to have high similarity values, and words that we don't associate together to have low similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "# get the vectors associated with two sample words\n",
    "vec_a = model.wv['beach']\n",
    "vec_b = model.wv['ocean']\n",
    "\n",
    "# find the n most similar terms to a given query vector\n",
    "print(model.wv.similar_by_vector(vec_a, topn=3))\n",
    "\n",
    "# show the distance between those vectors\n",
    "print(1 - cosine(vec_a, vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if you try to get the vector associated with a word that doesn't exist, an error springs!\n",
    "word = 'cats_in_trees'\n",
    "\n",
    "try:\n",
    "  model.wv[word]\n",
    "except KeyError:\n",
    "  print('! Word missing from vocabulary !', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find all words in the model\n",
    "words = list(model.wv.vocab.keys())\n",
    "\n",
    "words[:100] # show just the first hundred words in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Word Vector Fields\n",
    "\n",
    "In the code above, we examined the simialarity between two word vectors. In what follows, we'll visualize the similarity between hundreds of words at once. To do so, we'll reduce the vector representation of each word to just two dimensions, then we'll create a visualization that renders each word at its two-dimensional position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# visualize the field of terms fed as input\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "def plot_word_field(model, method='umap', skip=0, max_words=1000, jitter=0, margin=0.05, figsize=(20, 14), min_dist=0.1):\n",
    "  '''\n",
    "  Given a gensim.model instance, plot the positions of terms\n",
    "  '''\n",
    "  if method not in ['tsne', 'umap']: raise Exception(' ! Requested method is not supported')\n",
    "  words = get_model_words(model, max_words=max_words, skip=skip)\n",
    "  if not words: raise Exception(' ! No words were found in model -- exiting')\n",
    "  word_vectors = np.array([model.wv[word] for word in words]) # array of vecs, one per word\n",
    "  print(' * creating layout for', len(word_vectors), 'words')\n",
    "  if method == 'umap':\n",
    "    X = UMAP(n_neighbors=5, min_dist=min_dist).fit_transform(word_vectors) # X.shape = len(words), 2\n",
    "  elif method == 'tsne':\n",
    "    X = TSNE(n_components=2).fit_transform(word_vectors) # X.shape = len(words), 2\n",
    "  plot_words(X, words, figsize=figsize, jitter=jitter, margin=margin)\n",
    "\n",
    "  \n",
    "def get_model_words(model, max_words=1000, skip=0):\n",
    "  '''Get the words from `model`. If `max_words` is provided, return up to that many words'''\n",
    "  words = list(model.wv.vocab.keys()) # find all words in the model\n",
    "  if max_words: # get the n most popular words if user requested max words\n",
    "    word_to_count = {w: model.wv.vocab[w].count for w in words}\n",
    "    sorted_by_count = sorted(word_to_count.items(), key=operator.itemgetter(1))\n",
    "    sorted_by_count.reverse()\n",
    "    words = [i[0] for i in sorted_by_count[skip:skip+max_words]]\n",
    "  return words\n",
    "\n",
    "\n",
    "def plot_words(X, words, jitter=False, figsize=(10,6), margin=0.5, labels={}):\n",
    "  '''Given `X` where shape = words,2 and list of strings `words` plot the words at positions in `X`'''\n",
    "  plt.figure(figsize=figsize)\n",
    "  words = [plt.text(X[word_idx][0], X[word_idx][1], word) for word_idx, word in enumerate(words)]\n",
    "  if jitter: adjust_text(words, lim=jitter)\n",
    "  if labels.get('x', False): plt.xlabel(labels['x'])\n",
    "  if labels.get('y', False): plt.ylabel(labels['y'])\n",
    "  # set the axis ranges\n",
    "  x_vals = [w.get_position()[0] for w in words]\n",
    "  y_vals = [w.get_position()[1] for w in words]\n",
    "  plt.xlim(( min(x_vals)-margin, max(x_vals)+margin ))\n",
    "  plt.ylim(( min(y_vals)-margin, max(y_vals)+margin ))\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = plot_word_field(model, skip=5000, max_words=500, jitter=0, min_dist=0.25, method='umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Terms Related to a Concept\n",
    "\n",
    "Previously, we found and compared vectors for individual words. We can also compare vectors realted to concepts or groups of related words. \n",
    "\n",
    "In this example, we create two lists containing words that we've determined relate to a concept. In our case we have `veggie_words` and `meat_words`. The code below will create a list of meat words & a list veggie words. Then it determines the word vector for each word in each concept list, finds the centroid of the word vectors for each concept, and creates a new list of other words that have a high similarity to each concept.  \n",
    "\n",
    "<img src='images/centroid.jpg'>\n",
    "\n",
    "<div style='text-align: center'><a href='http://mathwords.com/c/centroid.htm'>Image by Bruce Simmons</a></div>\n",
    "\n",
    "In the end, this should return a list of words that relate to each identified concept; in our case either meat or veggie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def find_centroid(words):\n",
    "  '''Given a list of words, get the centroid of those word's vectors'''\n",
    "  vecs = np.vstack([model.wv[w] for w in words if w in model.wv])\n",
    "  sums = np.array([ np.sum(vecs[:,idx]) for idx, i in enumerate(range(vecs[0].shape[0])) ])\n",
    "  return sums / vecs[0].shape[0]\n",
    "\n",
    "\n",
    "def find_similar_by_vec(vec, n=50):\n",
    "  '''Return the words for the `n` most similar words to a query vector'''\n",
    "  words, sims = zip(*model.wv.similar_by_vector(vec, topn=n))\n",
    "  curated = set()\n",
    "  for i in words:\n",
    "    if i.lower() not in curated:\n",
    "      curated.add(i.lower())\n",
    "  curated = list(curated)\n",
    "  if n:\n",
    "    return curated[:n]\n",
    "  return curated\n",
    "\n",
    "\n",
    "def find_similar_by_words(words, n=50):\n",
    "  '''Return the words for the `n` most similar words to a list of query words'''\n",
    "  centroid = find_centroid(words)\n",
    "  return find_similar_by_vec(centroid, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create lists of words in a conceptual category        \n",
    "veggie_words = ['asparagus', 'artichoke', 'avocado', 'beets', 'broccoli', 'carrot', 'celery', 'cauliflower', 'cucumber', 'eggplant', 'kale', 'lentils', 'lettuce', 'mushroom', 'olive', 'onion', 'pea', 'potato', 'salad', 'spinach', 'squash', 'tomato', 'turnip', 'yam', 'zucchini']\n",
    "meat_words = ['bacon', 'beef', 'chicken', 'crab', 'duck', 'goose', 'meat', 'meatball', 'mutton', 'offal', 'partridge', 'pheasant', 'pork', 'quail', 'rabbit', 'turkey', 'veal', 'venison']\n",
    "\n",
    "other_veggie_words = find_similar_by_words(veggie_words)\n",
    "other_meat_words = find_similar_by_words(meat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "other_meat_words\n",
    "#other_veggie_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting relationships between concepts\n",
    "\n",
    "Again we can reduce our vector representation to two demensions so we can plot our concepts.\n",
    "\n",
    "For this example, rather than plotting our words based on thier individual word vectors, we are plotting words based on their relation to our two concepts: meat or veggie. \n",
    "\n",
    "In the plot below, the x-axis represents the 'veggieness' of each word and the y-axis represents the 'meatiness' of each word. A higher value means a greater similarity to the concept. So a higher x value would imply the word is more similar to our orginal concept of `veggie_words` and a higher y value wold imply the word is more similar to our concept `meat_words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as dist\n",
    "\n",
    "def plot_distances_to_concepts(words, vec_one, vec_two, jitter=0, labels={}):\n",
    "  '''Plot each item in `words` accordint to its distance from `vec_one` and `vec_two`'''\n",
    "  # 2D array where subarrays = [dist_from_vec_one, dist_from_vec_two]\n",
    "  words = [w for w in words if w in model.wv]\n",
    "  distances = [[1-dist(model.wv[w], vec_one), 1-dist(model.wv[w], vec_two)] for w in words]\n",
    "  plot_words(np.vstack(distances), words, jitter=jitter, labels=labels) \n",
    "\n",
    "# find the centroids for each cluster of words\n",
    "veggie_centroid = find_centroid(veggie_words)\n",
    "meat_centroid = find_centroid(meat_words)\n",
    "\n",
    "# plot the words in the concept space\n",
    "words = other_veggie_words + other_meat_words\n",
    "plot_distances_to_concepts(words, veggie_centroid, meat_centroid, jitter=4, labels={'x': 'vegginess', 'y': 'meatiness'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Word2Vec Models with Gensim\n",
    "\n",
    "In the previous examples we used the pre-built model obtained from Google. We can also build a custom model using our own text documents. In the examples below, we are downloading three documents from Project Gutenberg, creating a list with each word from each document, then generating a model from that word list using the gensim function `Word2Vec()`. We've also saved our model in a file called _word2vec.model_. Now we can use this model in other applications or share it with colleagues. \n",
    "\n",
    "Finally, we plot the results in two dimensions. We limit our plot to 1000 words to decrease processing time. The full model can also be plotted if desired, but note that this will take much longer to run and dispaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import requests as r\n",
    "import os\n",
    "\n",
    "def download_text(url, out_dir='texts'):\n",
    "  '''Download the content at `url` to the present directory'''\n",
    "  if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "  response = r.get(url, allow_redirects=True)\n",
    "  out_path = os.path.join(out_dir, os.path.basename(i))\n",
    "  open(out_path, 'wb').write(response.content)\n",
    "  print(' * downloaded', url)\n",
    "\n",
    "# specify the urls to one or more text files\n",
    "urls = [\n",
    "  'http://www.gutenberg.org/cache/epub/14/pg14.txt',\n",
    "  'http://www.gutenberg.org/cache/epub/25/pg25.txt',\n",
    "  'http://www.gutenberg.org/cache/epub/48/pg48.txt',\n",
    "]\n",
    "\n",
    "# download each of the text files specified above\n",
    "for i in urls: download_text(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "\n",
    "# generate a list of lists, where sublists contain words from a file (in order)\n",
    "word_lists = []\n",
    "for i in glob.glob('texts/*'):\n",
    "  with codecs.open(i) as f:\n",
    "    word_lists.append(f.read().lower().split())\n",
    "\n",
    "# build a model with the custom word lists\n",
    "model = Word2Vec(word_lists, size=100, window=5, min_count=20, workers=4)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the words\n",
    "plot_word_field(model, method='umap', min_dist=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Word Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "max_words = 1000\n",
    "skip = 1000\n",
    "\n",
    "words = get_model_words(model, max_words=max_words, skip=skip)\n",
    "vecs = [model.wv[i] for i in words]\n",
    "\n",
    "Z = linkage(vecs, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(max_words/4, 8))\n",
    "dendrogram(Z, leaf_rotation=90.0, leaf_font_size=14.0, labels=words)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
